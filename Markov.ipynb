{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eef5ef4",
   "metadata": {},
   "source": [
    "The management of conferences is handled for leagues such as the American championship (e.g., East/West), while in leagues like the Italian one, a single group or multiple groups (e.g., Group A, B) can be specified depending on the context. This is controlled through the team.csv and conference.csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4ac7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Leggi i file CSV\n",
    "plays = pd.read_csv(\"\", # path of Read File Output  \n",
    "                     usecols=[\"match_id\", \"set_number\", \"point_id\", \"home_team_score\", \"visiting_team_score\", \"team_touch_id\",\n",
    "                              \"team\", \"serving_team\", \"point_won_by\", \"skill\", \"evaluation_code\", \n",
    "                              \"team_id\", \"home_team_id\", \"visiting_team_id\", \"player_id\", \"player_name\", \n",
    "                              \"start_zone\", \"end_zone\", \"start_coordinate_x\", \"start_coordinate_y\", \n",
    "                              \"end_coordinate_x\", \"end_coordinate_y\", \"attack_code\",\"point\",\n",
    "                              \"home_setter_position\", \"visiting_setter_position\",\n",
    "                              \"home_player_id1\", \"home_player_id2\", \"home_player_id3\", \"home_player_id4\", \n",
    "                              \"home_player_id5\", \"home_player_id6\", \"home_p1\", \"home_p2\", \"home_p3\", \n",
    "                              \"home_p4\", \"home_p5\", \"home_p6\", \"visiting_p1\", \"visiting_p2\", \"visiting_p3\", \n",
    "                              \"visiting_p4\", \"visiting_p5\", \"visiting_p6\", \n",
    "                              \"visiting_player_id1\", \"visiting_player_id2\", \"visiting_player_id3\", \n",
    "                              \"visiting_player_id4\", \"visiting_player_id5\", \"visiting_player_id6\", \"code\", \"role\",\"split\"],\n",
    "                     na_values=[\"\"])\n",
    "\n",
    "# Leggi i team e conference\n",
    "team = pd.read_csv(\"/Users/acco/Desktop/Catene Di Markov/Python/data/team.csv\", dtype={\"team_id\": str})\n",
    "conference = pd.read_csv(\"/Users/acco/Desktop/Catene Di Markov/Python/data/conference.csv\", dtype={\"conference_id\": str})\n",
    "\n",
    "team_conf = (\n",
    "    plays\n",
    "    .drop_duplicates(subset=[\"match_id\"])  # Equivalent to dplyr::distinct\n",
    "    .rename(columns={\"home_team_id\": \"team_id_home\", \"visiting_team_id\": \"team_id_away\"})  # Rinomina le colonne\n",
    "    .merge(team, left_on=\"team_id_home\", right_on=\"team_id\", how=\"left\")  # Join per home team\n",
    "    .rename(columns={\"conference_id\": \"conf_id_home\"})  # Rinomina la colonna conference_id\n",
    "    .merge(team, left_on=\"team_id_away\", right_on=\"team_id\", how=\"left\")  # Join per away team\n",
    "    .rename(columns={\"conference_id\": \"conf_id_away\"})  # Rinomina la colonna conference_id\n",
    "    .loc[:, [\"match_id\", \"team_id_home\", \"team_id_away\", \"conf_id_home\", \"conf_id_away\"]]  # Seleziona le colonne finali\n",
    ")\n",
    "\n",
    "# Filtra i giocatori con id valido\n",
    "player = plays[plays['player_id'].notna() & (plays['player_id'] != 'unknown player')] \\\n",
    "    .groupby(['player_id', 'player_name', 'team_id']) \\\n",
    "    .size() \\\n",
    "    .reset_index(name=\"n\") \\\n",
    "    .sort_values(by='n', ascending=False) \\\n",
    "    .drop_duplicates(subset='player_id', keep='first') \\\n",
    "    .drop(columns=\"n\")\n",
    "\n",
    "# Visualizza i dati elaborati\n",
    "print(team_conf.head())\n",
    "print(player.head())\n",
    "\n",
    "# Salva i risultati in file CSV\n",
    "team_conf.to_csv('team_conf.csv', index=False)\n",
    "player.to_csv('player.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd69db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumpaste(x, sep=\".\"):\n",
    "    # Joins elements with a \".\" separator, maintaining the cumulative structure\n",
    "    return [x[0]] + [sep.join(x[:i+1]) for i in range(1, len(x))]\n",
    "\n",
    "# Create the `contact` DataFrame\n",
    "contact = (\n",
    "    plays\n",
    "    .merge(team_conf, on=\"match_id\", how=\"left\")  # left_join\n",
    "    .assign(\n",
    "        team_id_offense=lambda df: df[\"team_id\"],\n",
    "        team_id_defense=lambda df: np.where(\n",
    "            df[\"team_id\"] == df[\"team_id_home\"], df[\"team_id_away\"], df[\"team_id_home\"]\n",
    "        ),\n",
    "        conf_id_offense=lambda df: np.where(\n",
    "            df[\"team_id\"] == df[\"team_id_home\"], df[\"conf_id_home\"], df[\"conf_id_away\"]\n",
    "        ),\n",
    "        conf_id_defense=lambda df: np.where(\n",
    "            df[\"team_id\"] == df[\"team_id_home\"], df[\"conf_id_away\"], df[\"conf_id_home\"]\n",
    "        ),\n",
    "        serve_receive=lambda df: np.where(df[\"team_id\"] == df['home_team_id'], \"H\", \"V\"),\n",
    "        is_volley_end=lambda df: df[\"point\"].shift(-1).fillna(True).astype(bool) | \n",
    "                                  (df[\"team_touch_id\"] != df[\"team_touch_id\"].shift(-1)),\n",
    "        abbrev=lambda df: np.where(\n",
    "            df[\"point\"], \"P\",\n",
    "            np.where(\n",
    "                df[\"skill\"].isna() | df[\"skill\"].str.contains(\"Unknown\"), np.nan,\n",
    "                np.where(\n",
    "                    df[\"skill\"] == \"Serve\", \"SV\",\n",
    "                    np.where(\n",
    "                        df[\"skill\"] == \"Attack\", \"A\" + df[\"attack_code\"].astype(str),\n",
    "                        df[\"skill\"].str[0] + df[\"evaluation_code\"].astype(str)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        player_id_lead_1=lambda df: df[\"player_id\"].shift(-1),\n",
    "        skill_lead_1=lambda df: df[\"skill\"].shift(-1),\n",
    "        evaluation_code_lead_1=lambda df: df[\"evaluation_code\"].shift(-1)\n",
    "    )\n",
    "    .dropna(subset=[\"abbrev\", \"serving_team\"])  # Filter rows\n",
    ")\n",
    "\n",
    "def create_state(group):\n",
    "    # List of skill abbreviations\n",
    "    abbrev_list = group['abbrev'].tolist()\n",
    "    # Create states by cumulatively concatenating abbreviations\n",
    "    states = cumpaste(abbrev_list, sep=\".\")\n",
    "    # Assign the full state string to each row\n",
    "    group['state'] = [f\"{group['serve_receive'].iloc[0]}_{state}\" for state in states]\n",
    "    group['num_contacts'] = len(group['abbrev'])\n",
    "    return group\n",
    "\n",
    "# Apply `create_state` to each group\n",
    "contact = contact.groupby([\"match_id\", \"point_id\", \"team_touch_id\", \"point\"], group_keys=False).apply(create_state)\n",
    "\n",
    "# Reset index\n",
    "contact = contact.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd12525",
   "metadata": {},
   "outputs": [],
   "source": [
    "contact['match_numeric'] = contact['match_id'].astype('category').cat.codes\n",
    "\n",
    "contact['rally_id'] = contact.apply(\n",
    "    lambda x: f\"{x['match_numeric']}_{x['set_number']}_{x['point_id']}\",\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ef849",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for rid, group in contact.groupby('rally_id'):\n",
    "    # group contiene tutte le righe di un singolo rally, già in ordine\n",
    "    states = group['state'].tolist()\n",
    "    \n",
    "    # Scorriamo gli stati in coppia (stato_i, stato_i+1)\n",
    "    for i in range(len(states) - 1):\n",
    "        current_state = states[i]\n",
    "        next_state    = states[i+1]\n",
    "        rows.append((rid, current_state, next_state))\n",
    "\n",
    "transitions = pd.DataFrame(rows, columns=['rally_id', 'current_state', 'next_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2baa7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = Counter(zip(transitions['current_state'], transitions['next_state']))\n",
    "state_counts = Counter(transitions['current_state'])\n",
    "\n",
    "# Estraggo l’elenco di tutti gli stati (current e next)\n",
    "all_states = set(transitions['current_state']).union(set(transitions['next_state']))\n",
    "all_states = sorted(all_states)  # ordiniamoli per coerenza\n",
    "\n",
    "# Creiamo una matrice di transizione come DataFrame\n",
    "transition_matrix = pd.DataFrame(\n",
    "    0.0, \n",
    "    index=all_states, \n",
    "    columns=all_states\n",
    ")\n",
    "\n",
    "# Riempiamo la matrice con le probabilità\n",
    "for (cs, ns), c in pair_counts.items():\n",
    "    transition_matrix.loc[cs, ns] = c / state_counts[cs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix.loc['H_P', 'H_P'] = 1.0\n",
    "transition_matrix.loc['V_P', 'V_P'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'transition_matrix' is a DataFrame with rows/columns as chain states\n",
    "\n",
    "# 1. List of all states\n",
    "states_list = list(transition_matrix.index)\n",
    "\n",
    "# 2. Initialize absorption probability vectors:\n",
    "#    vH = prob. of absorption into 'H_P' from state s\n",
    "#    vV = prob. of absorption into 'V_P' from state s\n",
    "vH = {s: 0.0 for s in states_list}\n",
    "vV = {s: 0.0 for s in states_list}\n",
    "\n",
    "# Set absorbing states manually\n",
    "vH['H_P'] = 1.0\n",
    "vV['H_P'] = 0.0\n",
    "vH['V_P'] = 0.0\n",
    "vV['V_P'] = 1.0\n",
    "\n",
    "# 3. Iterative function to update absorption probabilities\n",
    "def iterate_absorption(vH, vV, trans_mat):\n",
    "    new_vH, new_vV = {}, {}\n",
    "    for s in states_list:\n",
    "        if s in ['H_P', 'V_P']:\n",
    "            new_vH[s] = vH[s]\n",
    "            new_vV[s] = vV[s]\n",
    "        else:\n",
    "            row = trans_mat.loc[s].values\n",
    "            col_names = trans_mat.columns\n",
    "            new_vH[s] = np.sum(row * np.array([vH[t] for t in col_names]))\n",
    "            new_vV[s] = np.sum(row * np.array([vV[t] for t in col_names]))\n",
    "    return new_vH, new_vV\n",
    "\n",
    "# 4. Convergence parameters\n",
    "epsilon = 1e-8\n",
    "max_iter = 1000\n",
    "\n",
    "# 5. Iterate until convergence\n",
    "for i in range(max_iter):\n",
    "    old_vH, old_vV = vH.copy(), vV.copy()\n",
    "    vH, vV = iterate_absorption(vH, vV, transition_matrix)\n",
    "\n",
    "    diff_H = max(abs(vH[s] - old_vH[s]) for s in states_list)\n",
    "    diff_V = max(abs(vV[s] - old_vV[s]) for s in states_list)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration {i}: diff_H = {diff_H:.2e}, diff_V = {diff_V:.2e}\")\n",
    "    \n",
    "    if diff_H < epsilon and diff_V < epsilon:\n",
    "        print(f\"Converged at iteration {i}\")\n",
    "        break\n",
    "\n",
    "# 6. Create DataFrame with final results\n",
    "results = [(s, vH[s], vV[s]) for s in states_list]\n",
    "results_df = pd.DataFrame(results, columns=['state', 'prob_H', 'prob_V'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
